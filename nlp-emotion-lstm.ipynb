{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:17.468348Z",
     "iopub.status.busy": "2022-02-12T15:41:17.467871Z",
     "iopub.status.idle": "2022-02-12T15:41:17.481860Z",
     "shell.execute_reply": "2022-02-12T15:41:17.480542Z",
     "shell.execute_reply.started": "2022-02-12T15:41:17.468297Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:17.485426Z",
     "iopub.status.busy": "2022-02-12T15:41:17.484431Z",
     "iopub.status.idle": "2022-02-12T15:41:26.803989Z",
     "shell.execute_reply": "2022-02-12T15:41:26.802685Z",
     "shell.execute_reply.started": "2022-02-12T15:41:17.485379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\milon\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\milon\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\milon\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\milon\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\milon\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# #this will download nltk in your $HOME/nltk_data directory\n",
    "# nltk.download(\"all\")\n",
    "!pip install nltk\n",
    "#sudo python -m nltk.downloader -d /usr/share/nltk_data all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:26.808224Z",
     "iopub.status.busy": "2022-02-12T15:41:26.807831Z",
     "iopub.status.idle": "2022-02-12T15:41:26.936239Z",
     "shell.execute_reply": "2022-02-12T15:41:26.934658Z",
     "shell.execute_reply.started": "2022-02-12T15:41:26.808190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak Data\t: 20000\n",
      "Training\t: 16000\n",
      "Testing\t\t: 2000\n",
      "Validation\t: 2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data preprocessing\n",
    "\n",
    "def traintestval(dir):\n",
    "    data = pd.read_csv(dir, header=None)\n",
    "    data = data.rename(columns = {0:\"sentences\", 1:\"classes\"})\n",
    "    category = pd.get_dummies(data.classes)\n",
    "    data = pd.concat([data, category], axis=1)\n",
    "    data = data.drop(columns='classes')\n",
    "    x = data['sentences'].values\n",
    "    y = data[['anger',\t'fear',\t'joy',\t'love',\t'sadness',\t'surprise']].values\n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = traintestval('train_lstm.csv')\n",
    "x_test, y_test = traintestval('test_lstm.csv')\n",
    "x_val, y_val = traintestval('val_lstmcsv.csv')\n",
    "\n",
    "print(f\"Banyak Data\\t: {len(x_train)+len(x_test)+len(x_val)}\\nTraining\\t: {len(x_train)}\\nTesting\\t\\t: {len(x_test)}\\nValidation\\t: {len(x_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:26.940016Z",
     "iopub.status.busy": "2022-02-12T15:41:26.939360Z",
     "iopub.status.idle": "2022-02-12T15:41:26.948399Z",
     "shell.execute_reply": "2022-02-12T15:41:26.947046Z",
     "shell.execute_reply.started": "2022-02-12T15:41:26.939948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak Data\t: 20000\n",
      "Training\t: 16000\n",
      "Validation\t: 4000\n",
      "Presentase\t: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# validaton set less than 20% from overall dataset, we can merge testing set and validation set, cz we only need data test here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "newx_val = np.concatenate([x_test,x_val])\n",
    "newy_val = np.concatenate([y_test,y_val]) \n",
    "print(f\"Banyak Data\\t: {len(x_train)+len(x_test)+len(x_val)}\\nTraining\\t: {len(x_train)}\\nValidation\\t: {len(newx_val)}\\nPresentase\\t: {len(newx_val)/(len(x_train)+len(newx_val))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:26.950864Z",
     "iopub.status.busy": "2022-02-12T15:41:26.949771Z",
     "iopub.status.idle": "2022-02-12T15:41:26.974517Z",
     "shell.execute_reply": "2022-02-12T15:41:26.973396Z",
     "shell.execute_reply.started": "2022-02-12T15:41:26.950814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['আমি অপমানিত বোধ করি।',\n",
       "        'আমি খুব নিরাশ বোধ থেকে খুব অভিশপ্ত আশাবাদী হতে পারি শুধুমাত্র যত্নশীল এবং জাগ্রত একজনের আশেপাশে থাকা থেকে',\n",
       "        'আমি পোস্ট করার জন্য একটি মিনিট ধরছি আমি লোভী ভুল বোধ করছি', ...,\n",
       "        'আমি সামগ্রিকভাবে শক্তিশালী এবং ভাল বোধ করি',\n",
       "        'আমি মনে করি এটি একটি অভদ্র মন্তব্য ছিল এবং আমি খুশি যে টি',\n",
       "        'আমি অনেক কিছু জানি কিন্তু আমি খুব বোকা বোধ করি কারণ আমি এটি চিত্রিত করতে পারি না'],\n",
       "       dtype=object),\n",
       " array(['আমি বরং পচা বোধ করছি তাই আমি এখন খুব উচ্চাভিলাষী নই।',\n",
       "        'আমি আমার ব্লগ আপডেট করছি কারণ আমি খারাপ বোধ করছি',\n",
       "        'আমি কখনই তাকে আমার থেকে আলাদা করি না কারণ আমি কখনই চাই না যে সে তার সাথে লজ্জিত বোধ করুক',\n",
       "        ...,\n",
       "        'যারা একই জিনিস অনুভব করছেন তাদের জন্য এই তথ্যটি শেয়ার করা আমি গুরুত্বপূর্ণ মনে করি',\n",
       "        'আমি সত্যিই অনুভব করি যে আপনি যদি কিছু সম্পর্কে যথেষ্ট উত্সাহী হন এবং নিজের প্রতি সত্য থাকেন তবে আপনি সফল হবেন',\n",
       "        'আমার মনে হচ্ছে আমি অনলাইনে বা এমনকি একটি দেখতে যেকোনো সুন্দর মেক আপ কিনতে চাই'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case folding\n",
    "\n",
    "for data in x_train:\n",
    "    data = data.lower()\n",
    "\n",
    "for data in newx_val:\n",
    "    data = data.lower()\n",
    "\n",
    "x_train, newx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:26.977278Z",
     "iopub.status.busy": "2022-02-12T15:41:26.976589Z",
     "iopub.status.idle": "2022-02-12T15:41:27.043375Z",
     "shell.execute_reply": "2022-02-12T15:41:27.042242Z",
     "shell.execute_reply.started": "2022-02-12T15:41:26.977230Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93fdb3203403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# filtering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# collecting english stopword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# collecting english stopword \n",
    "stopwords_lists = set(stopwords.words('bengali'))\n",
    "\n",
    "def filtering(group_of_texts):\n",
    "    data = []\n",
    "    for text in group_of_texts:\n",
    "        data.append([word for word in text if not word in stopwords_lists])\n",
    "    return data\n",
    "\n",
    "# removing stopwords\n",
    "train_without_stopword = filtering(train_token)\n",
    "val_without_stopword = filtering(val_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:27.045594Z",
     "iopub.status.busy": "2022-02-12T15:41:27.045300Z",
     "iopub.status.idle": "2022-02-12T15:41:27.861159Z",
     "shell.execute_reply": "2022-02-12T15:41:27.860131Z",
     "shell.execute_reply.started": "2022-02-12T15:41:27.045553Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def sentence_lem(sentence):\n",
    "    words = []\n",
    "    for word in sentence:\n",
    "        words.append(WordNetLemmatizer().lemmatize(word))\n",
    "    return words\n",
    "\n",
    "def lemmatization(group_of_texts):\n",
    "    for sentence in group_of_texts:\n",
    "        sentence = sentence_lem(sentence)\n",
    "    return group_of_texts\n",
    "\n",
    "x_train = lemmatization(train_without_stopword)\n",
    "x_val = lemmatization(val_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:27.864068Z",
     "iopub.status.busy": "2022-02-12T15:41:27.863375Z",
     "iopub.status.idle": "2022-02-12T15:41:27.921207Z",
     "shell.execute_reply": "2022-02-12T15:41:27.919040Z",
     "shell.execute_reply.started": "2022-02-12T15:41:27.864020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\milon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenizing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist \n",
    "\n",
    "\n",
    "#removing number\n",
    "#removing punctuation\n",
    "#remove whitespace leading & trailing\n",
    "#remove multiple whitespace into single whitespace\n",
    "def tokenize(group_of_texts):\n",
    "    token = []\n",
    "    for text in group_of_texts:\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        text = text.strip()\n",
    "        text = re.sub('\\s+',' ',text)\n",
    "        temp = word_tokenize(text)\n",
    "        token.append(temp)\n",
    "    return token\n",
    "\n",
    "train_token = tokenize(x_train)\n",
    "val_token = tokenize(newx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:37.955921Z",
     "iopub.status.busy": "2022-02-12T15:41:37.955637Z",
     "iopub.status.idle": "2022-02-12T15:41:38.444812Z",
     "shell.execute_reply": "2022-02-12T15:41:38.443809Z",
     "shell.execute_reply.started": "2022-02-12T15:41:37.955890Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='-')\n",
    "tokenizer.fit_on_texts(x_train) \n",
    "tokenizer.fit_on_texts(x_val)\n",
    " \n",
    "sequence_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(x_val)\n",
    " \n",
    "padded_train = pad_sequences(sequence_train) \n",
    "padded_test = pad_sequences(sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-12T15:41:27.925668Z",
     "iopub.status.idle": "2022-02-12T15:41:27.926269Z",
     "shell.execute_reply": "2022-02-12T15:41:27.925986Z",
     "shell.execute_reply.started": "2022-02-12T15:41:27.925927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,  323,    4, 1867],\n",
       "        [   0,    0,    0, ...,  913,  234,   34],\n",
       "        [   0,    0,    0, ...,  298,    4,    9],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,   71,    4,    5],\n",
       "        [   0,    0,    0, ...,  173,    7, 2134],\n",
       "        [   0,    0,    0, ...,   12,   23,   10]]),\n",
       " array([[   0,    0,    0, ...,  203,   66, 2687],\n",
       "        [   0,    0,    0, ...,  132,  671,  156],\n",
       "        [   0,    0,    0, ...,  191,    8,    5],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,  220,   11,    5],\n",
       "        [   0,    0,    0, ...,   45,  422, 1045],\n",
       "        [   0,    0,    0, ...,  501, 1041,   41]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train, padded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:53.373882Z",
     "iopub.status.busy": "2022-02-12T15:41:53.373530Z",
     "iopub.status.idle": "2022-02-12T15:41:53.676826Z",
     "shell.execute_reply": "2022-02-12T15:41:53.675841Z",
     "shell.execute_reply.started": "2022-02-12T15:41:53.373848Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow import optimizers\n",
    "\n",
    "# build up model\n",
    "\n",
    "model = Sequential([\n",
    "                    Embedding(input_dim=5000, output_dim=16),\n",
    "                    LSTM(64),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dense(64, activation='relu'),\n",
    "                    Dense(6, activation='softmax')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:41:57.172438Z",
     "iopub.status.busy": "2022-02-12T15:41:57.171622Z",
     "iopub.status.idle": "2022-02-12T15:41:57.185637Z",
     "shell.execute_reply": "2022-02-12T15:41:57.184541Z",
     "shell.execute_reply.started": "2022-02-12T15:41:57.172373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          80000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                20736     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,702\n",
      "Trainable params: 117,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:42:00.979547Z",
     "iopub.status.busy": "2022-02-12T15:42:00.978974Z",
     "iopub.status.idle": "2022-02-12T15:42:00.994084Z",
     "shell.execute_reply": "2022-02-12T15:42:00.992981Z",
     "shell.execute_reply.started": "2022-02-12T15:42:00.979515Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import optimizers\n",
    "\n",
    "# compile mode\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:42:04.882686Z",
     "iopub.status.busy": "2022-02-12T15:42:04.882357Z",
     "iopub.status.idle": "2022-02-12T15:42:04.889108Z",
     "shell.execute_reply": "2022-02-12T15:42:04.887398Z",
     "shell.execute_reply.started": "2022-02-12T15:42:04.882654Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# membuat early stopping\n",
    "es = EarlyStopping(monitor = 'val_loss',\n",
    "                   verbose = 0,\n",
    "                   patience = 2,\n",
    "                   mode = 'min',\n",
    "                   restore_best_weights = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:42:08.576363Z",
     "iopub.status.busy": "2022-02-12T15:42:08.575706Z",
     "iopub.status.idle": "2022-02-12T15:42:26.661912Z",
     "shell.execute_reply": "2022-02-12T15:42:26.661009Z",
     "shell.execute_reply.started": "2022-02-12T15:42:08.576327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 2000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ad93c1a9bd7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpadded_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewy_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 2000\n  y sizes: 4000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "history = model.fit(padded_train, y_train,epochs = 50,validation_data = (padded_test, newy_val),verbose=2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:42:52.312775Z",
     "iopub.status.busy": "2022-02-12T15:42:52.312471Z",
     "iopub.status.idle": "2022-02-12T15:42:55.265081Z",
     "shell.execute_reply": "2022-02-12T15:42:55.264041Z",
     "shell.execute_reply.started": "2022-02-12T15:42:52.312738Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(padded_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(padded_test, newy_val, verbose=0)\n",
    "print('> Accuracy Train: %.3f' % (train_acc * 100))\n",
    "print('> Accuracy Test: %.3f' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T15:42:59.397088Z",
     "iopub.status.busy": "2022-02-12T15:42:59.396771Z",
     "iopub.status.idle": "2022-02-12T15:42:59.784610Z",
     "shell.execute_reply": "2022-02-12T15:42:59.783576Z",
     "shell.execute_reply.started": "2022-02-12T15:42:59.397058Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "plt.subplot(121)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# plot accuracy during training\n",
    "plt.subplot(122)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='training')\n",
    "plt.plot(history.history['val_accuracy'], label='testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
